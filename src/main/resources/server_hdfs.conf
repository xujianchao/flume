hdfs.sources = r1 r2
hdfs.channels = c1 c2
hdfs.sinks = k2 hdfsSink
hdfs.sources.r1.type = com.ultrapower.flume.extend.source.http.HTTPSource
hdfs.sources.r1.bind = 10.0.0.225
hdfs.sources.r1.port = 8083
hdfs.sources.r1.channels = c1


hdfs.channels.c1.type = memory
hdfs.channels.c1.capacity = 100000000
hdfs.channels.c1.transactionCapacity = 10000000

hdfs.sinks.hdfsSink.type=hdfs
hdfs.sinks.hdfsSink.hdfs.path=hdfs://spark1:9000/flume/%y-%m-%d/%H%M/%S
hdfs.sinks.hdfsSink.hdfs.round = true
hdfs.sinks.hdfsSink.hdfs.roundValue = 10
hdfs.sinks.hdfsSink.hdfs.roundUnit = minute
hdfs.sinks.hdfsSink.hdfs.useLocalTimeStamp = true
hdfs.sinks.hdfsSink.hdfs.codeC = gzip
hdfs.sinks.hdfsSink.hdfs.fileType=CompressedStream  
hdfs.sinks.hdfsSink.channel=c1




#hdfs.channels.c1.type = com.ultrapower.flume.extend.channel.KafkaChannel
#hdfs.channels.c1.capacity = 100000
#hdfs.channels.c1.transactionCapacity = 100000
#hdfs.channels.c1.brokerList= 10.0.0.128:9092
#hdfs.channels.c1.topic=FLUME_MESSAGE
#hdfs.channels.c1.zookeeperConnect= 10.0.0.128:2181

hdfs.sources.r2.type = com.ultrapower.flume.extend.source.kafka.KafkaSource
hdfs.sources.r2.broker = 10.0.0.128:9092
hdfs.sources.r2.topic = FLUME_ORDER
hdfs.sources.r2.group = flumeConsumer
hdfs.sources.r2.partition = 0
hdfs.sources.r2.channels = c2
hdfs.channels.c2.type =memory
hdfs.channels.c2.capacity = 100000
hdfs.channels.c2.transactionCapacity = 100000
hdfs.sinks.k2.type =com.ultrapower.flume.extend.sink.http.HttpSink
hdfs.sinks.k2.port = 8082
hdfs.sinks.k2.channel = c2